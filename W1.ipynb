{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "data": {
            "text/plain": "\u003cFigure size 640x480 with 0 Axes\u003e"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "\u003cFigure size 480x480 with 1 Axes\u003e"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": "\"\"\"\nWorkbook analyzing DIGITS dataset from scikit-learn using K Nearest Neighbours Classifier\n\"\"\"\n\n\"\"\"\n1. Election of dataset: Digits dataset is going to be used.\nThis is a classification dataset where each data-point is a 8x8 image image of a digit.\n\"\"\"\n\nfrom sklearn.datasets import load_digits\ndigits \u003d load_digits()\n\n\"\"\"\n2. Description of the dataset\n\"\"\"\n# An example of the data:\nimport matplotlib.pyplot as plt\nplt.gray()\nplt.matshow(digits.images[12])\nplt.show()\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "(number of observations, number of features): (1797, 64)\nNum of digit \u00270\u0027 samples: 178\nNum of digit \u00271\u0027 samples: 182\nNum of digit \u00272\u0027 samples: 177\nNum of digit \u00273\u0027 samples: 183\nNum of digit \u00274\u0027 samples: 181\nNum of digit \u00275\u0027 samples: 182\nNum of digit \u00276\u0027 samples: 181\nNum of digit \u00277\u0027 samples: 179\nNum of digit \u00278\u0027 samples: 174\nNum of digit \u00279\u0027 samples: 180\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# data is a dictionary. We can then navigate through the dataset using the keys\ndigits.keys()\nX \u003d digits[\u0027data\u0027]\ny \u003d digits[\u0027target\u0027]\n\n# (number of observations, number of features) Note that num features is 64 because it\u0027s a 8x8 image\nprint(\"(number of observations, number of features): \"+ str(X.shape))\n\n# classes: (each one of the 10 class is a digit from 0 to 9\nclasses \u003d digits[\u0027target_names\u0027]\n\n# number of samples per class\nfor i in classes:\n    print(\"Num of digit \u0027\" + str(i) + \"\u0027 samples: \" + str(sum(y \u003d\u003d i)))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\nBest Estimator:\nKNeighborsClassifier(algorithm\u003d\u0027auto\u0027, leaf_size\u003d30, metric\u003d\u0027minkowski\u0027,\n           metric_params\u003dNone, n_jobs\u003dNone, n_neighbors\u003d1, p\u003d2,\n           weights\u003d\u0027uniform\u0027)\n\nParameters of best estimator:\n{\u0027n_neighbors\u0027: 1}\n\nTraining score: 0.9895615866388309\nTest score: 0.9861111111111112\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\"\"\"\nEXPERIMENTS\n\"\"\"\n\n# the dataset is split so that 80% of the data is used for training, and 20% for test.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2, random_state\u003d13)\n\n# K Nearest Neighbors Classifier is going to be used\nfrom sklearn.neighbors import KNeighborsClassifier\nmyKNN \u003d KNeighborsClassifier()\n\n# required imports for performing grid search cross validation and stratified k fold technique.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n# A param grid dictionary is created with the parameters to try in the cross-validation process\nparam_grid \u003d {\u0027n_neighbors\u0027: [1, 3, 5, 7, 9, 11, 13, 15]}\n\n# KNN estimator, accuracy as scoring, and the data will be split into 10 chunks, thus, it will take 10 iterations\nmyGSCV \u003d GridSearchCV(estimator\u003dmyKNN, param_grid\u003dparam_grid, scoring\u003d\u0027accuracy\u0027,\n                      cv\u003dStratifiedKFold(n_splits\u003d10, random_state\u003d3))\n\n# Training of the model\nmyGSCV.fit(X_train, y_train)\n\n# prediction (using best_estimator_ by default)\ny_pred \u003d myGSCV.predict(X_test)\n\n# Results\nprint(\"\\nBest Estimator:\\n\" + str(myGSCV.best_estimator_))  # best estimator\nprint(\"\\nParameters of best estimator:\\n\" + str(myGSCV.best_params_))   # parameters of the best estimator\nprint(\"\\nTraining score: \" + str(myGSCV.best_score_))  # training score for achieved with the best estimator\nprint(\"Test score: \" + str(myGSCV.score(X_test, y_test)))  # test score\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\nUsing Leave one out validation.\nNow, in each iteration of the cross validation process, all observations except one will be used for training.\nThis means that, in the first iteration, observation #1 is out, in second iteration, observation #2 is out, and so on.\nThus, there will be as many iterations as training samples (length of X_train), which means a high computational cost.\n\"\"\"\nfrom sklearn.model_selection import LeaveOneOut\nmyLOO \u003d GridSearchCV(estimator\u003dmyKNN, param_grid\u003dparam_grid, scoring\u003d\u0027accuracy\u0027, cv\u003dLeaveOneOut(), n_jobs\u003d-1)\n\n# Training of the model\nmyLOO.fit(X_train, y_train)\n\n# prediction (using best_estimator_ by default)\ny_predLOO \u003d myLOO.predict(X_test)\n\n# Results\nprint(\"\\nBest Estimator:\\n\" + str(myLOO.best_estimator_))   # best estimator\nprint(\"\\nParameters of best estimator:\\n\" + str(myLOO.best_params_))    # parameters of the best estimator\nprint(\"\\nTraining score: \" + str(myLOO.best_score_))  # training score for achieved with the best estimator\nprint(\"Test score: \" + str(myLOO.score(X_test, y_test)))  # test score\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\n5. Stratified K Fold.\nThere is a similar number of samples per each class, so, if this distribution is preserved when\nsplitting into training (X_train) and (X_test), stratification is not indispensable.\n\"\"\"\n\nprint(\u0027Classes distribution in Training\u0027)\nfor i in classes:\n    print(\"Num of digit \u0027\" + str(i) + \"\u0027 samples: \" + str(sum(y_train \u003d\u003d i)))\n\nprint(\u0027\\nClasses distribution in Test\u0027)\nfor i in classes:\n    print(\"Num of digit \u0027\" + str(i) + \"\u0027 samples: \" + str(sum(y_test \u003d\u003d i)))\n\n# KNN estimator, accuracy as scoring, and the data will be split into 10 chunks, thus, it will take 10 iterations\nfrom sklearn.model_selection import KFold\n\nmyGSCV_noStrat \u003d GridSearchCV(estimator\u003dmyKNN, param_grid\u003dparam_grid, scoring\u003d\u0027accuracy\u0027,\n                              cv\u003dKFold(n_splits\u003d10, random_state\u003d3))\n\n# Training of the model\nmyGSCV_noStrat.fit(X_train, y_train)\n\n# prediction (using best_estimator_ by default)\ny_pred_noStrat \u003d myGSCV_noStrat.predict(X_test)\n\n# Results. It\u0027s shown how the score is not affected when not using Stratified KFOLD\nprint(\"\\nBest Estimator:\\n\" + str(myGSCV_noStrat.best_estimator_))  # best estimator\nprint(\"\\nParameters of best estimator:\\n\" + str(myGSCV_noStrat.best_params_))   # parameters of the best estimator\nprint(\"\\nTraining score: \" + str(myGSCV_noStrat.best_score_))  # training score for achieved with the best estimator\nprint(\"Test score: \" + str(myGSCV_noStrat.score(X_test, y_test)))  # test score\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": true
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\n6. Distance Weights\n# uniform weights (default): all points in each neighborhood have same weight (used in exercise 3)\n# distance weights: points closer to the evaluated will have more influence.\n\"\"\"\nparam_grid \u003d {\u0027n_neighbors\u0027: [1, 3, 5, 7, 9, 11, 13, 15], \u0027weights\u0027: [\u0027uniform\u0027, \u0027distance\u0027]}\n# KNN estimator, accuracy as scoring, and the data will be split into 10 chunks, thus, it will take 10 iterations\nmyGSCV_w \u003d GridSearchCV(estimator\u003dmyKNN, param_grid\u003dparam_grid, scoring\u003d\u0027accuracy\u0027,\n                        cv\u003dStratifiedKFold(n_splits\u003d10, random_state\u003d3))\n\n# Training of the model\nmyGSCV_w.fit(X_train, y_train)\n\n# prediction (using best_estimator_ by default)\ny_pred_w \u003d myGSCV_w.predict(X_test)\n\n# Results. Note that distance weights are preferred\nprint(\"\\nBest Estimator:\\n\" + str(myGSCV_w.best_estimator_))  # best estimator\nprint(\"\\nParameters of best estimator:\\n\" + str(myGSCV_w.best_params_))   # parameters of the best estimator\nprint(\"\\nTraining score: \" + str(myGSCV_w.best_score_))  # training score for achieved with the best estimator\nprint(\"Test score: \" + str(myGSCV_w.score(X_test, y_test)))  # test score\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\"\"\"\n7. Testing different metrics.\nThe default metric used is minkowski. Now euclidean and manhattan metrics will be taken into account, thus, they will\nbe included in the param grid.\nAlso, for minkowski, a parameter \u0027p\u0027 can be adjusted, so the different values will also be included.\n\"\"\"\nparam_grid \u003d {\u0027n_neighbors\u0027: [1, 3, 5, 7, 9, 11, 13],\n              \u0027weights\u0027: [\u0027uniform\u0027, \u0027distance\u0027],\n              \u0027metric\u0027: [\u0027euclidean\u0027, \u0027manhattan\u0027, \u0027minkowski\u0027],\n              \u0027p\u0027: [2, 3, 4, 5, 6, 7, 8, 9]}\n\nmyGSCV_m \u003d GridSearchCV(estimator\u003dmyKNN, param_grid\u003dparam_grid, scoring\u003d\u0027accuracy\u0027,\n                        cv\u003dStratifiedKFold(n_splits\u003d10, random_state\u003d3))\n\n# Training of the model\nmyGSCV_m.fit(X_train, y_train)\n\n# prediction (using best_estimator_ by default)\ny_pred_m \u003d myGSCV_m.predict(X_test)\n\n# Results\nprint(\"\\nBest Estimator:\\n\" + str(myGSCV_m.best_estimator_))  # best estimator\nprint(\"\\nParameters of best estimator:\\n\" + str(myGSCV_m.best_params_))   # parameters of the best estimator\nprint(\"\\nTraining score: \" + str(myGSCV_m.best_score_))  # training score for achieved with the best estimator\nprint(\"Test score: \" + str(myGSCV_m.score(X_test, y_test)))  # test score\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}